{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        ""
      ],
      "metadata": {
        "id": "NovogfbdCUkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "nw6U79SUCrFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCxb9xs8BWvx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Define the base model and LoRA adapter\n",
        "base_model_id = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\n",
        "lora_adapter_id = \"huseyincavus/qwen3-14b-finance-lora\"\n",
        "merged_model_id = \"huseyincavus/qwen3-14b-finance-merged\"\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load the LoRA adapter and merge it with the base model\n",
        "model = PeftModel.from_pretrained(model, lora_adapter_id)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Push the merged model and tokenizer to the Hugging Face Hub\n",
        "model.push_to_hub(merged_model_id)\n",
        "tokenizer.push_to_hub(merged_model_id)"
      ]
    }
  ]
}