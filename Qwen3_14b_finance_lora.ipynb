{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3zJW6IlZhL9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the Model\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    dtype = None, # Will default to torch.bfloat16 if available\n",
        ")\n",
        "\n",
        "# 2. Configure LoRA Adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank of the adapters. A common choice.\n",
        "    lora_alpha = 16, # A scaling factor for the adapters.\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 42,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        ")\n",
        "\n",
        "print(\"Unsloth model configured for 4-bit LoRA fine-tuning!\")\n"
      ],
      "metadata": {
        "id": "FrYdrNcHaLAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Datasets and Merge them\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "def load_and_merge_finance_datasets():\n",
        "    print(\"Loading gbharti/wealth-alpaca_lora dataset...\")\n",
        "    wealth_ds = load_dataset(\"gbharti/wealth-alpaca_lora\", split=\"train\")\n",
        "\n",
        "    print(\"Loading Josephgflowers/Finance-Instruct-500k dataset...\")\n",
        "    finance_ds = load_dataset(\"Josephgflowers/Finance-Instruct-500k\", split=\"train\")\n",
        "\n",
        "    def preprocess_wealth_alpaca(example):\n",
        "        if example.get('input'):\n",
        "            example['instruction'] = f\"{example['instruction']}\\n{example['input']}\"\n",
        "        return {\"instruction\": example[\"instruction\"], \"output\": example[\"output\"]}\n",
        "\n",
        "    def preprocess_finance_instruct(example):\n",
        "        # The output should come from the 'assistant' column in the dataset\n",
        "        return {\"instruction\": example[\"user\"], \"output\": example[\"assistant\"]}\n",
        "\n",
        "    wealth_ds = wealth_ds.map(preprocess_wealth_alpaca, remove_columns=wealth_ds.column_names)\n",
        "    finance_ds = finance_ds.map(preprocess_finance_instruct, remove_columns=finance_ds.column_names)\n",
        "\n",
        "    print(\"Merging the datasets...\")\n",
        "    merged_dataset = concatenate_datasets([wealth_ds, finance_ds])\n",
        "    return merged_dataset\n",
        "\n",
        "merged_dataset = load_and_merge_finance_datasets()"
      ],
      "metadata": {
        "id": "nZF2BS9LIq4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data for Qwen3 ChatML format\n",
        "\n",
        "# We create a new column 'text' that contains the formatted prompt.\n",
        "# SFTTrainer will then use this column for training.\n",
        "def formatting_prompts_func(example):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
        "    ]\n",
        "    # The tokenizer formats the messages into the required ChatML string.\n",
        "    # We don't tokenize here, just create the formatted text string.\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    return { \"text\": text }\n",
        "\n",
        "dataset = merged_dataset.map(formatting_prompts_func)\n",
        "\n",
        "print(\"\\n--- Formatted Dataset Example ---\")\n",
        "print(dataset[0][\"text\"])"
      ],
      "metadata": {
        "id": "HFlZlRSofHSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA and Start Training\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4, # Effective batch size = 2 * 4 = 8\n",
        "    warmup_steps = 10,\n",
        "    max_steps = 300,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 1,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 42,\n",
        "    output_dir = \"outputs\",\n",
        ")\n",
        "\n",
        "# --- Initialize Trainer ---\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\", # Point trainer to our formatted 'text' column\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = training_args,\n",
        ")\n",
        "\n",
        "# --- Start Fine-tuning ---\n",
        "print(\"Starting the fine-tuning process...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")"
      ],
      "metadata": {
        "id": "vAeqKPaB3N5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference and Saving the Model\n",
        "\n",
        "print(\"\\n--- Running Inference ---\")\n",
        "from transformers import pipeline\n",
        "\n",
        "# Use Unsloth's fast inference pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Create a test prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What are the main risks associated with investing in emerging markets?\"},\n",
        "]\n",
        "\n",
        "# Get the response\n",
        "outputs = pipe(messages, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.95)\n",
        "print(outputs[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "dUG6jbQMckeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ed4c699"
      },
      "source": [
        "# Save the Adapters and Push to Hugging Face Hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Save the fine-tuned LoRA adapters\n",
        "print(\"\\n--- Saving LoRA Adapters ---\")\n",
        "model.save_pretrained(\"qwen3_30b_finance_lora\")\n",
        "tokenizer.save_pretrained(\"qwen3_30b_finance_lora\")\n",
        "print(\"Model adapters saved to 'qwen3_30b_finance_lora'\")\n",
        "\n",
        "# Log in to Hugging Face Hub\n",
        "notebook_login()\n",
        "\n",
        "# Push the model adapters and tokenizer to the Hub\n",
        "repo_name = \"huseyincavus/qwen3-30b-finance-lora\"\n",
        "\n",
        "print(f\"\\n--- Pushing LoRA Adapters to Hugging Face Hub ({repo_name}) ---\")\n",
        "model.push_to_hub(repo_name, token = True)\n",
        "tokenizer.push_to_hub(repo_name, token = True)\n",
        "print(\"Model adapters and tokenizer pushed to Hugging Face Hub!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
